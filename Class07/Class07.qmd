---
title: "Class 7: Machine Learning"
author: "Sarah Tareen"
format: gfm
editor: visual
---

## Example of K-means clustering

First step is to make up some data with a known structure, so we know what the answer should be. \`\`\`

```{r}
#Make two clear subgroups of points
tmp <- c( rnorm(30, mean = -3), rnorm(30, mean = 3))
x <- cbind(x = tmp, y = rev(tmp))
plot(x)
```

Now we have some structured data in `x`. Let's see if k-means is able to identify the two groups.

```{r}
k <- kmeans(x, centers = 2, nstart = 20)
k
```

Let's explore `k`:

```{r}
# size shows you how points are in each cluster
k$size
```

```{r}
# this shows the center of each cluster
k$centers
```

```{r}
# which point belongs to which cluster
k$cluster
```

```{r}
# gives a different color to each cluster based on which points are in it 
plot(x, col = k$cluster)
```

Now we can add the clusters centers:

```{r}
plot(x, col = k$cluster)
# this command adds to the plot, calling plot again will just make a new plot, can change the color and shape of these added points as well
points(k$centers, col = 'blue', pch = 15)
```

This method's disadvantage is that you need to know the number of clusters initially because k will run with the number you tell it to.

Here is an example when we select the wrong number of clusters for k-means:

```{r}
# 3 clusters does not make sense
k3 <- kmeans(x, centers = 3, nstart = 20)
plot(x, col = k3$cluster)
```

# Example of Hierarchical Clustering

Let's use the same data as before, which we stored in `x`. We will use the `hclust` function.

```{r}
#dist calculates the distances between all the point and this is the input for hclust
clustering <- hclust(dist(x))
clustering
```

```{r}
# we can see two distinct groups with numbers 1-30 on the left and 31-60 on the right
plot(clustering)
```

Let's add a horizontal line:

```{r}
# We can use this line to define our clusters
# Don't overexplain the data to find patterns that may not be there or overclassify too specific to your data set. 
# We should be able to apply logic to other data sets
plot(clustering)
abline(h = 10, col = "pink")
```

To get our results (i.e., membership vector) we need to "cut" the tree. The function for doing that is `cutree()`.

```{r}
subgroups <- cutree(clustering, h = 10)
subgroups
```

```{r}
plot(x, col = subgroups)
```

You can also "cut" your tree with the number of clusters you want:

```{r}
 subgroups2 <- cutree(clustering, k = 2)
 subgroups2
```

# Principal Component Analysis (PCA)

## PCA of UK food data

First we want to read the data.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
head(x)
```

\>Q1. How many rows and columns are in your new data frame named `x`? What R functions could you use to answer this questions?

```{r}
# dim() returns rows and columns
dim(x)
```

There are 17 rows and 4 columns.

\>Q2. Which approach to solving the 'row-names problem' mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I used the approach that set `rownames=1` because it was more simple and easier to understand. One might be more robust since if you run `x <- x[,-1]` multiple times you can remove more columns if desired but its also easier to choose one of the columns to set `rownames` to which deletes that column as well.

Now we can generate some basic visualizations.

\>Q3. Changing what optional argument in the above **`barplot()`** function results in the following plot? Changing the argument `col` resulted in this colorful plot based on the food categories of the rows.

```{r}
barplot(as.matrix(x), col = rainbow( nrow(x) ))
```

Let's refine our barplot:

```{r}
barplot(as.matrix(x), col = rainbow( nrow(x) ), beside = T)
```

Other visualizations that can be useful...

\>Q5. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The code is making a plot that gives a different color to each category in the rows. This pairwise plot is plotting each variable against each other. For example, in the top row, the first plot is England vs. Wales, the second is England vs. Scotland, and the third is England vs Northern Ireland. These plots can be repeated in other rows. If a point lies along the diagonal of a plot, it means that there is a high correlation for the point between the two variables being plotted against each other.

```{r}
pairs(x, col = rainbow( nrow(x) ))
```

\>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The main difference is that in Northern Ireland people are more likely to eat potatoes and drink soft drinks in higher proportions than the other three countries in the UK.

## PCA to the rescue

Let's apply PCA (principal components analysis). For that, we need to use the command `prcomp()`. This function expects the transpose of our data.

```{r}
pca <- prcomp(t(x))
summary(pca)
# in just two dimensions we can see 96% of the data we observed with the 17 dimensions
# we don't need all 17 dimensions, we can just use these two
```

Let's plot the PCA results:

```{r}
# This plot just shows us the 4 components and their variances. Its just a visual representation of the summary. 
plot(pca)
```

We need to access the results of the PCA analysis.

```{r}
# tells you what is in pca, x are the coordinates of the principal components
attributes(pca)
```

We can explore the `pca$x` dataframe:

```{r}
pca$x
```

Plotting:

```{r}
#variation in the x axis is much higher than the y axis because the x axis is PCA 1
plot( x=pca$x[,1], y=pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
```

\>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
#these two give the same names
#colnames(x)
#rownames(pca$x)
plot( x=pca$x[,1], y=pca$x[,2] )
#can choose specific colors instead of just random rainbow colors
colors_countries <- c('orange', 'pink', 'blue', 'green')
text( x=pca$x[,1], y=pca$x[,2], colnames(x), col = colors_countries)
```

Let's see how each variable affected the PCs using something called loading scores which can be found in the `rotation` component of the PCA.

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

\>Q9. Generate a similar 'loadings plot' for PC2. What two food groups feature prominantely and what does PC2 mainly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

The two food groups that feature prominently are fresh potatoes and soft drinks. PC2 mainly tells us about how Northern Ireland peoples' food choices differ from the rest of the countries in the UK.

# PCA of a RNA-Seq Dataset

First step as always is loading the data.

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
```

**\>Q10**: How many genes and samples are in this data set?

```{r}
dim( rna.data )
```

I have 100 genes and 10 samples.

Let's apply PCA:

```{r}
pca_rna = prcomp( t(rna.data) )
#Just one dimension PCA1 explains 99% of the differences in the data. 
summary(pca_rna)
```

Let's plot the principal components 1 and 2.

```{r}
plot( pca_rna$x[,1], pca_rna$x[,2], xlab = 'PCA1', ylab = 'PCA2')
```

```{r}
colnames(rna.data)

cols_samples <- c(rep('blue', 5), rep('red', 5))
cols_samples

#can see very clearly the groups of wildtype and knockout gene 
#can use this as quality control in an experiment
plot( pca_rna$x[,1], pca_rna$x[,2], xlab = 'PCA1', ylab = 'PCA2', col = cols_samples)
```

```{r}
barplot(pca_rna$rotation[,1])
```

```{r}
#can see over and underexpression of genes by sorting
sort(pca_rna$rotation[,1])
```
