---
title: "Lab8_mini_project"
format: gfm
editor: visual
author: Sarah Tareen
---

# 1. Exploratory Data Analysis

## Preparing the Data

Let's load the data from the breast mass samples:

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

We can remove the diagnosis from the data frame because that gives us the answers.

```{r}
wisc.data <- wisc.df[,-1]
```

Let's save the diagnosis column as a vector that will be useful for plotting later.

```{r}
diagnosis <- wisc.df[,1]
```

## Exploratory data analysis

\>**Q1**. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

There are 569 observations in this dataset, represented by the rows.

\>**Q2**. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

There are 212 malignant observations.

\>Q3. How many variables/features in the data are suffixed with `_mean`?

```{r}
#the numbers are the column numbers with the substring
grep("_mean", colnames(wisc.data))
```

There are 10 variables (columns) that are suffixed with `_mean`.

# 2. Principal Component Analysis

## Performing PCA

Check if you need to scale the data.

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Let's do PCA analysis using the `prcomp()` function.

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

Let's see a summary of the PCA analysis.

```{r}
# Look at summary of results
summary(wisc.pr)
```

\>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

We can see from the summary in the proportion of variance that PC1 has 0.4427 proportion of original variance of the data set.

\>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

By looking at the PCA summary, we can look at the cumulative proportions and we see that PC1 to 3 cumulatively makes up 72.636% of the total variance. Therefore, we need the first three PCs to get at least 70% of the original variance in the data.

\>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Looking at the summary, we see that PC1 to 7 make up 91.01% of the total variance.

## Interpreting PCA Results

Let's visualize the PCA analysis using a `biplot`.

```{r}
biplot(wisc.pr)
```

\>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot has the column names and sample numbers all in the same graph. The column names seem to all be connected in the middle. There are also additional axes on the right and the top which are unlabeled. This plot is very difficult to understand as there is so much text all centered in a small space which makes it impossible to see all the data at once.

This plot is very bad so lets make another type of plot.

```{r}
# Scatter plot observations by components 1 and 2
fdiag <- as.factor(diagnosis)
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = fdiag, xlab = "PC1", ylab = "PC2")
```

\>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Scatter plot observations by components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = fdiag, xlab = "PC1", ylab = "PC3")
```

I noticed that the there is an axis for each PC and it is much easier to tell which samples are more closely related. You can also easily tell what the outlier samples are.

In the plot above, we can see that PC1 is differentiating between the malignant and benign samples.

### ggplot2

Let's use `ggplot2` to make even better plots. First we need to change our PCA analysis to a data frame and add the diagnosis column to used for the color aesthetic.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

Now we can make the plot.

```{r}
# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## Variance Explained

We can look at more plots that show the variance of each PC. First we can look at variance of each principal component.

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
pve <- pr.var/sum(pr.var)
```

Now we can create the scree plot.

```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

We can also make a barplot from the same data.

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

\>Q9. For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`? This tells us how much this original feature contributes to the first PC.

```{r}
sort(wisc.pr$rotation[,1])
```

The concave.points_mean is the most important for PC1 and its component in the variance of PC1 is -0.26085376.

# 3. Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
wisc.hclust
```

\>**Q10.** Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="red", lty=2, h=18)
```

The height at which the clustering model has 4 clusters is 18.

We can use `cutree()` so that our tree has 4 clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
plot(wisc.hclust.clusters)
```

Lets compare the assigned clusters to the actual diagnoses.

```{r}
# we can see that cluster 1 corresponds to malignant cells while cluster 3 corresponds to benign cells
table(wisc.hclust.clusters, diagnosis)
```

## Using different methods

We can use different methods of combining points in `hclust`.

```{r}
# this first plot is using "complete" as default
plot(wisc.hclust)

wisc.hclust.s <- hclust(data.dist, method = "single")
plot(wisc.hclust.s)

wisc.hclust.a <- hclust(data.dist, method = "average")
plot(wisc.hclust.a)

wisc.hclust.w <- hclust(data.dist, method = "ward.D2")
plot(wisc.hclust.w)
```

\>Q12. Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning.

I liked the `ward.D2` method because I can see the clusters more easily and it looks less messy.

# 4. Combining methods

## Clustering on PCA results

We want to see if PCA will improve `hclust` clustering.

```{r}
pca.dist = dist(wisc.pr$x[, 1:7])
wisc.pr.hclust = hclust(pca.dist, method="ward.D2")
plot(wisc.pr.hclust)
```

Let's see if the two main branches of this dendrogram are indicating the diagnoses.

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=fdiag)
```

Let's use the distance along the first 7 PCs for clustering.

```{r}
pca.dist.7 = dist(wisc.pr$x[, 1:7])
wisc.pr.hclust.7 = hclust(pca.dist.7, method="ward.D2")
plot(wisc.pr.hclust.7)
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr.hclust.clusters)
```

Now let's compare our model to the actual diagnoses.

\>Q13. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

\>Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the `table()` function to compare the output of each model (`wisc.km$cluster` and `wisc.hclust.clusters`) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

I believe that when we combined the PCA and hierarchical clustering models together this model did a better job at distinguishing the malignant samples since there is a higher number of malignant samples in cluster 1 of `wisc.pr.hclust.clusters` than `wisc.hclust.clusters`. However I believe the `hclust` model that separated the data into 4 clusters did a better job at distinguishing the number of benign samples. Overall the PCA and `hclust` model is more intuitive since we cut the tree into 2 clusters and it is easier to tell which cluster correlates with which diagnosis.

# 6. Prediction

We can compare old and new cancer cell data by using our PCA model to predict.

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=fdiag)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

\>Q16. Which of these new patients should we prioritize for follow up based on your results?

We need to prioritize the patients who have the most variation according to PC1 because these are the ones who are clustered close to the malignant samples from our previous PCA model.
